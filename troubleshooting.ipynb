{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "475da3ba",
      "metadata": {
        "id": "475da3ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\n",
        "                      f'cuda:{torch.cuda.current_device()}') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.set_default_device(device)"
      ],
      "metadata": {
        "id": "HdJiM9eiik6T"
      },
      "id": "HdJiM9eiik6T",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1ca9b88",
      "metadata": {
        "id": "f1ca9b88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def BetaSchedule(n_steps, start=1e-4, end=0.02):\n",
        "    \"\"\"\n",
        "    Generates a beta schedule for the forward process.\n",
        "    \"\"\"\n",
        "    return np.linspace(start, end, n_steps)\n",
        "\n",
        "def ForwardProcess(timesteps, initial_data):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates noised data using the forward process. Takes one random timestep\n",
        "    generate a 1D noised data and store it in the noised_data array. This is repeated\n",
        "    for each data point in the initial_data array.\n",
        "    \"\"\"\n",
        "\n",
        "    ndata = len(initial_data)\n",
        "    noised_data = np.zeros(ndata)\n",
        "    noise = np.zeros(ndata)\n",
        "    beta = BetaSchedule(timesteps)\n",
        "    alpha = 1 - beta\n",
        "\n",
        "    for i in range(ndata):\n",
        "\n",
        "        time = np.random.randint(len(beta))\n",
        "        noise[i] = np.random.normal(0,1)\n",
        "        noised_data[i] = initial_data[i]*np.sqrt(np.prod(alpha[:i])) + (1 - np.prod(alpha[:i]))*noise[i]\n",
        "\n",
        "    SaveCSV(noised_data, \"noised_data\")\n",
        "    SaveCSV(noise, \"noise\")\n",
        "\n",
        "    #return  noised_data, noise\n",
        "\n",
        "def GenerateTwoDeltas(ndata):\n",
        "\n",
        "    print(f\"Generating two deltas distribution with {ndata} data points...\")\n",
        "\n",
        "    return np.concatenate([np.ones(ndata//2),-np.ones(ndata//2)])\n",
        "\n",
        "def GenerateNoisedData(timesteps, ndata, initial_distribution):\n",
        "\n",
        "    data = initial_distribution(ndata)\n",
        "\n",
        "    print(\"forward process started...\")\n",
        "\n",
        "    ForwardProcess(timesteps, data)\n",
        "\n",
        "    print(\"forward process ended...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0e42b1ac",
      "metadata": {
        "id": "0e42b1ac"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def CreateDataloader(data,target):\n",
        "\n",
        "  data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "  target_tensor = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "\n",
        "  data_ds = TensorDataset(data_tensor,target_tensor)\n",
        "  data_dl = DataLoader(data_ds, batch_size=100, shuffle=True)\n",
        "\n",
        "  return data_dl\n",
        "\n",
        "def Preprocessing(data, target):\n",
        "\n",
        "  train_data_, test_data, train_target_, test_target = train_test_split(data, target, test_size=0.2)\n",
        "\n",
        "  train_data, valid_data, train_target, valid_target = train_test_split(train_data_, train_target_, test_size=0.2)\n",
        "\n",
        "  train_dl = CreateDataloader(train_data, train_target)\n",
        "  valid_dl = CreateDataloader(valid_data, valid_target)\n",
        "  test_dl = CreateDataloader(test_data, test_target)\n",
        "\n",
        "  return train_dl, valid_dl, test_dl\n",
        "\n",
        "def SaveCSV(data, name):\n",
        "\n",
        "   df_data = pd.DataFrame(data)\n",
        "   df_data.to_csv( name + \".csv\", index=False, header=False)\n",
        "\n",
        "def LoadCSV(name):\n",
        "\n",
        "   return pd.read_csv( name + \".csv\", header=None).values.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c9a5401b",
      "metadata": {
        "id": "c9a5401b",
        "outputId": "01f2b934-f83a-4b17-b573-15e6f4efc859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating two deltas distribution with 1000 data points...\n",
            "forward process started...\n",
            "forward process ended...\n"
          ]
        }
      ],
      "source": [
        "GenerateNoisedData(300, 1000, GenerateTwoDeltas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dcc763a9",
      "metadata": {
        "id": "dcc763a9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, output_size,n_hidden_layers,depht):\n",
        "    super().__init__()\n",
        "\n",
        "    l = [nn.Linear(input_size,depht), nn.ReLU()]\n",
        "    i = 0\n",
        "    while i < n_hidden_layers:\n",
        "      l.append(nn.Linear(depht, depht))\n",
        "      l.append(nn.ReLU())\n",
        "      i+=1\n",
        "    l.append(nn.Linear(depht, output_size))\n",
        "    self.model_list = nn.ModuleList(l)\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.model_list:\n",
        "      x = layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9f91c06b",
      "metadata": {
        "id": "9f91c06b"
      },
      "outputs": [],
      "source": [
        "def Train(learning_rate,model,num_epochs,train_dl,valid_dl, patience=5, min_delta=0.001,):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    loss_hist_train = np.zeros(num_epochs)\n",
        "    loss_hist_valid = np.zeros(num_epochs)\n",
        "\n",
        "    # Variables for early stopping\n",
        "\n",
        "    #best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()  # Set model to training mode\n",
        "      train_loss = 0.0\n",
        "\n",
        "      for x_batch, y_batch in train_dl:\n",
        "\n",
        "        x = x_batch.view(x_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "        y = y_batch.view(y_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "\n",
        "        pred = model(x)\n",
        "            #Define loss function\n",
        "        loss = loss_fn(pred, y)\n",
        "            #Backpropagation\n",
        "        loss.backward()\n",
        "            #Apply gradient to the weights\n",
        "        optimizer.step()\n",
        "            #Make gradients zero\n",
        "        optimizer.zero_grad()\n",
        "        loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
        "\n",
        "      for x_batch, y_batch in valid_dl:\n",
        "\n",
        "        x = x_batch.view(x_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "        y = y_batch.view(y_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss_hist_valid[epoch] += loss.item()*y_batch.size(0)\n",
        "\n",
        "      loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "      loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
        "\n",
        "      #if loss_hist_valid[epoch] < best_loss - min_delta:\n",
        "        #best_loss = loss_hist_valid[epoch]\n",
        "        #epochs_no_improve = 0\n",
        "        #best_model_state = model.state_dict()  # Save the best model\n",
        "      #else:\n",
        "      #  epochs_no_improve += 1\n",
        "\n",
        "      #if epochs_no_improve >= patience:\n",
        "        #print(f'Early stopping triggered after {epoch+1} epochs!')\n",
        "        #model.load_state_dict(best_model_state)  # Restore best model\n",
        "        #final_epoch = epoch\n",
        "        #break\n",
        "\n",
        "    #loss_hist_train = loss_hist_train[:final_epoch+1]\n",
        "    #loss_hist_valid = loss_hist_valid[:final_epoch+1]\n",
        "\n",
        "    return loss_hist_train, loss_hist_valid\n",
        "\n",
        "def TrainModel(num_epochs, learning_rate):\n",
        "\n",
        "    model = FeedForward(input_size=1,output_size=1,n_hidden_layers=2,depht=100).to(device)\n",
        "\n",
        "    noised_data, noise = LoadCSV(\"noised_data\"), LoadCSV(\"noise\")\n",
        "    train_dl, valid_dl, test_dl = Preprocessing(noised_data, noise)\n",
        "\n",
        "    loss_hist_train,loss_hist_valid = Train(learning_rate=learning_rate, model=model, num_epochs=num_epochs,\n",
        "                                           train_dl=train_dl, valid_dl=valid_dl\n",
        "                                           )\n",
        "\n",
        "    return model, loss_hist_train, loss_hist_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4e1c1535",
      "metadata": {
        "id": "4e1c1535",
        "outputId": "00c51f2d-5d20-4287-ad98-0d23cbb111c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected a 'cuda' device type for generator but found 'cpu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-606873774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_hist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_hist_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3739330622.py\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoised_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     loss_hist_train,loss_hist_valid = Train(learning_rate=learning_rate, model=model, num_epochs=num_epochs,\n\u001b[0m\u001b[1;32m     75\u001b[0m                                            \u001b[0mtrain_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                            )\n",
            "\u001b[0;32m/tmp/ipython-input-3739330622.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(learning_rate, model, num_epochs, train_dl, valid_dl, patience, min_delta)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_droplast\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             yield from torch.randperm(n, generator=generator).tolist()[\n\u001b[1;32m    191\u001b[0m                 \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected a 'cuda' device type for generator but found 'cpu'"
          ]
        }
      ],
      "source": [
        "model, loss_hist_train, loss_hist_valid = TrainModel(100, learning_rate=0.01)\n",
        "\n",
        "plt.plot(loss_hist_train, label='train')\n",
        "plt.plot(loss_hist_valid, label='valid')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeedForward(input_size=1,output_size=1,n_hidden_layers=1,depht=100).to(device)\n",
        "\n",
        "noised_data, noise = LoadCSV(\"noised_data\"), LoadCSV(\"noise\")\n",
        "\n",
        "#train_dl, valid_dl, test_dl = Preprocessing(noised_data, noise)\n",
        "\n",
        "\n",
        "#for x_batch, y_batch in train_dl:\n",
        "  #x = x_batch.view(x_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "  #y = y_batch.view(y_batch.size(0), -1).detach().clone().requires_grad_(True)\n",
        "\n",
        "  #plt.hist(model(x).detach().numpy())\n",
        "  #print(x.device)\n",
        "\n",
        "  #break\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcIhP8IPMD6e",
        "outputId": "864abebc-c73c-4294-e7bd-cf632124f98f"
      },
      "id": "ZcIhP8IPMD6e",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8fmq4EkRjUn"
      },
      "id": "M8fmq4EkRjUn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}